{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from __future__ import print_function\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.misc import fromimage, toimage\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# desired size of the output image\n",
    "# gpu 512 이상 시 out of memory error 발생\n",
    "imsize = 512 if use_cuda else 128\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "def image_loader(image_name, transform=None, max_size=None, shape=None):\n",
    "    image = Image.open(image_name)\n",
    "    \n",
    "    if max_size is not None:\n",
    "        scale = max_size / max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
    "    \n",
    "    if shape is not None:\n",
    "        image = image.resize(shape, Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image)\n",
    "        \n",
    "    image = Variable(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    \n",
    "    return image.type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 여기는 코드에 있어서 써보려고 했지만 필요없더라고~~\n",
    "# 이거 쓰면 이상하게 나왔어~\n",
    "def match_histogram(content_img, generated_img):    \n",
    "    oldshape = generated_img.shape\n",
    "    source = generated_img.ravel()\n",
    "    template = content_img.ravel()\n",
    "    \n",
    "    s_values, bin_idx, s_counts = np.unique(source, return_inverse=True, return_counts=True)\n",
    "    t_values, t_counts = np.unique(template, return_counts=True)\n",
    "    \n",
    "    s_quantiles = np.cumsum(s_counts).astype(np.float64)\n",
    "    s_quantiles /= s_quantiles[-1]\n",
    "    t_quantiles = np.cumsum(t_counts).astype(np.float64)\n",
    "    t_quantiles /= t_quantiles[-1]\n",
    "    \n",
    "    interp_t_values = np.interp(s_quantiles, t_quantiles, t_values)\n",
    "    \n",
    "    return interp_t_values[bin_idx].reshape(oldshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def orginal_color_transform(content_img, generated_img, hist_match=0, mode='YCbCr'):\n",
    "    content_img = content_img.squeeze(0)\n",
    "    content_img = content_img.data.cpu().numpy()\n",
    "    #generated_img = Variable(generated_img)\n",
    "    generated_img = generated_img.squeeze(0)\n",
    "    generated_img = generated_img.data.cpu().numpy()\n",
    "    \n",
    "    content_img = fromimage(toimage(content_img, mode='RGB'), mode=mode)\n",
    "    generated_img = fromimage(toimage(generated_img, mode='RGB'), mode=mode)\n",
    "    \n",
    "    if hist_match == 1:\n",
    "        for channel in range(3):\n",
    "            generated_img[:, :, channel] = match_histogram(generated_img[:, :, channel], content_img[:, :, channel])\n",
    "    else:\n",
    "        # 여기가 핵심이야~ 여기가 바꾸는 곳이야~\n",
    "        generated_img[ :, :, 1:] = content_img[ :, :,  1:]\n",
    "        \n",
    "    generated_img = fromimage(toimage(generated_img, mode=mode), mode='RGB')\n",
    "    content_img = fromimage(toimage(content_img, mode=mode), mode='RGB')\n",
    "    generated_img = transform(generated_img)\n",
    "    \n",
    "    generated_img = Variable(generated_img).cuda()\n",
    "    generated_img = generated_img.unsqueeze(0)\n",
    "    return generated_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target, weight):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach() * weight\n",
    "        self.weight = weight\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = self.criterion(input * self.weight, self.target)\n",
    "        self.output = input\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, retain_graph=True):\n",
    "        self.loss.backward(retain_graph=retain_graph)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "\n",
    "    def forward(self, input):\n",
    "        a, b, c, d = input.size()\n",
    "        features = input.view(a * b, c * d)\n",
    "        G = torch.mm(features, features.t())\n",
    "        return G.div(a * b * c * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target, weight):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = target.detach() * weight\n",
    "        self.weight = weight\n",
    "        self.gram = GramMatrix()\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.output = input.clone()\n",
    "        self.G = self.gram(input)\n",
    "        self.G.mul_(self.weight)\n",
    "        self.loss = self.criterion(self.G, self.target)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, retain_graph=True):\n",
    "        self.loss.backward(retain_graph=retain_graph)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG19를 Transfer Learning에 사용\n",
    "cnn = models.vgg19(pretrained=True).features\n",
    "\n",
    "# move it to the GPU if possible:\n",
    "if use_cuda:\n",
    "    cnn = cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# desired depth layers to compute style/content losses :\n",
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "\n",
    "def get_style_model_and_losses(cnn, style_img, content_img,\n",
    "                               style_weight=1000, content_weight=1,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    # just in order to have an iterable access to or list of content/syle\n",
    "    # losses\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    model = nn.Sequential()  # the new Sequential module network\n",
    "    gram = GramMatrix()  # we need a gram module in order to compute style targets\n",
    "\n",
    "    # move these modules to the GPU if possible:\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        gram = gram.cuda()\n",
    "\n",
    "    i = 1\n",
    "    for layer in list(cnn):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            name = \"conv_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "\n",
    "            if name in content_layers:\n",
    "                # add content loss:\n",
    "                target = model(content_img).clone()\n",
    "                content_loss = ContentLoss(target, content_weight)\n",
    "                model.add_module(\"content_loss_\" + str(i), content_loss)\n",
    "                content_losses.append(content_loss)\n",
    "\n",
    "            if name in style_layers:\n",
    "                # add style loss:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                style_loss = StyleLoss(target_feature_gram, style_weight)\n",
    "                model.add_module(\"style_loss_\" + str(i), style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            name = \"relu_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "\n",
    "            if name in content_layers:\n",
    "                # add content loss:\n",
    "                target = model(content_img).clone()\n",
    "                content_loss = ContentLoss(target, content_weight)\n",
    "                model.add_module(\"content_loss_\" + str(i), content_loss)\n",
    "                content_losses.append(content_loss)\n",
    "\n",
    "            if name in style_layers:\n",
    "                # add style loss:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                style_loss = StyleLoss(target_feature_gram, style_weight)\n",
    "                model.add_module(\"style_loss_\" + str(i), style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        if isinstance(layer, nn.MaxPool2d):\n",
    "            name = \"pool_\" + str(i)\n",
    "            model.add_module(name, layer)  # ***\n",
    "\n",
    "    return model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer (LBFGS)\n",
    "def get_input_param_optimizer(input_img):\n",
    "    input_param = nn.Parameter(input_img.data)\n",
    "    optimizer = optim.LBFGS([input_param])\n",
    "    return input_param, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step : 300\n",
    "# style weight = 1000 / content weight = 1\n",
    "def train_style_transfer(cnn, content_img, style_img, input_img, num_steps=150,\n",
    "                       style_weight=1000, content_weight=1):\n",
    "    print('Starting style transfer!!')\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn, style_img, content_img, style_weight, content_weight)\n",
    "    input_param, optimizer = get_input_param_optimizer(input_img)\n",
    "    \n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "        def steps():\n",
    "            input_param.data.clamp_(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            model(input_param)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.backward()\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(\"run {}: Style Loss : {:4f} Content Loss: {:4f}\".format(run, style_score.data[0], content_score.data[0]))\n",
    "\n",
    "            return style_score + content_score\n",
    "\n",
    "        optimizer.step(steps)\n",
    "\n",
    "    # a last correction...\n",
    "    input_param.data.clamp_(0, 1)\n",
    "\n",
    "    return input_param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style or Content image 변경 시 여기서 변경\n",
    "content_img = image_loader(\"images/Content/content.png\", transform, max_size=imsize)\n",
    "color_img = image_loader(\"images/Style/color.jpg\", transform, shape=[content_img.size(3), content_img.size(2)])\n",
    "style_img = image_loader(\"images/Style/style.jpg\", transform, shape=[content_img.size(3), content_img.size(2)])\n",
    "input_img = content_img.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting style transfer!!\n",
      "run [50]: Style Loss : 1.558883 Content Loss: 2.138358\n",
      "run [100]: Style Loss : 0.301498 Content Loss: 1.707018\n",
      "run [150]: Style Loss : 0.157995 Content Loss: 1.557527\n",
      "Starting style transfer!!\n",
      "run [50]: Style Loss : 3.795955 Content Loss: 6.441866\n",
      "run [100]: Style Loss : 1.076885 Content Loss: 6.035984\n",
      "run [150]: Style Loss : 0.689826 Content Loss: 5.704264\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "color_img = orginal_color_transform(color_img, content_img)\n",
    "color_output = train_style_transfer(cnn, content_img, color_img, input_img)\n",
    "input_img = content_img.clone()\n",
    "output = train_style_transfer(cnn, content_img, style_img, input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save Output Image\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "color_output = Variable(color_output)\n",
    "output = Variable(output)\n",
    "output = orginal_color_transform(color_output, output)\n",
    "\n",
    "output = output.data.cpu().numpy()\n",
    "output = torch.from_numpy(output)\n",
    "\n",
    "torchvision.utils.save_image(output, 'images/Output/ouput-comb.jpg')\n",
    "#output = orginal_color_transform(content_img, output)\n",
    "#torchvision.utils.save_image(output, 'images/Output/ouput-color.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
